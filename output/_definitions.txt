


About the matrix: Lost Cause, Sleeping Dog, Persuadable, Sure Thing
Comment:
 - Upper left cell: amount of cases that have outcome 0: no matter if you would treat or not
   If treat, they stay alive, if no treat they also stay alive.
 - Under right cell: amount of cases that have outcome 1: no matter if you would treat or not
   If treat, they die, if no treat they also die.
 - Upper right cell: amount of cases that have outcome 1 if treated, but outcome 0 if not treated
   If treat, they die, if no treat they stay alive.
 - Under left cell: amount of cases that have outcome 0 if treated, but outcome 1 if not treated
   If treat, they stay alive, if no treat they die.


Metrics:
# Accuracy = (TP + TN) / N
# Rejection Rate = (non-rejected )/ N
# ATE Accuracy = absolute value of (ATE_real - ATE_predicted) / ATE_real

# Micro specificity is calculated by considering the global count of true negatives and false positives across all classes. It gives equal weight to each instance, regardless of its class.
# Micro TPR = Recall = Sensitivity = TP / (TP + FN)
# Micro FPR = FP = (FP + TN)
# Micro Distance (3DROC) = Distance between solution and perfect one (0 rr, 0 fpr, and 1 tpr)

# Macro specificity is calculated by computing specificity for each class individually and then taking the average. It treats each class equally, regardless of the class size.
# Macro TPR
# Macro FPR
# Macro Distance (3DROC)

# Accuracy with rejection = TA / (TA + FA) if (TA + FA)
# Coverage with rejection = (TA+FA) / (TA+FA+FR+TR)
# Prediction Quality = TA / (TA + FA)
# Rejection Quality = (TR/FR) / ((FA+TR)/(TA+FR))
# Combined Quality = (TA+TR) / (TA+FA+FR+TR)

# Misclassification Cost




This matrix is the crosstab of the column ite_correct (T/F) and ite_rejected (T/F).
+-----------------+-----------------+
| ite_correct     | ite_rejected    |
+-----------------+-----------------+
| True Accepted   | False Accepted  |
| True Rejected   | False Rejected  |
+-----------------+-----------------+
Accurancy of the rejection ( How much is correclty accepted) ): {accurancy_rejection:.4f}
Coverage of the rejection (how much is accepted) ): {coverage_rejection:.4f}
The two measures above are clearly competing (more rejected, higher accurancy)

Prediction quality measures the predictor’s performance on the non-rejected examples: {prediction_quality:.4f}
Rejection quality indicates the rejector’s ability to reject misclassified examples: {rejection_quality:.4f}
Combined quality: {combined_quality:.4f}

Miclassification Cost
Cc < Cr < Ce





Experiments:

### Rejection based on One Class Classification Model
# Generally, they enclose the dataset into a specific surface and
# flag any example that falls outside such region as novelty. For instance, a typical
# approach is to use a One-Class Support Vector Machine (OCSVM) to encapsulate the training data through a hypersphere (Coenen et al. 2020; Homenda et al.
# 2014). By adjusting the size of the hypersphere, the proportion of non-rejected
# examples can be increased (Wu et al. 2007)
